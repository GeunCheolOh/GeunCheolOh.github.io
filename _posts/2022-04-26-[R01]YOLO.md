---
layout: post
title:  초보가 초보에게 논문리뷰 - YOLO v1
categories: review
tag: [YOLO, review]
use_math: true
toc: true
author_profile: false
date: 2022-04-16 00:23:20 +0200
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


오늘 읽어볼 논문은 2016년 발표된 "You Only Look Once"입니다. 객체 탐지 분야에서 새로운 지평을 열고, v5까지 발전하며 널리 이용되는 모습을 보여주었던 모델이죠.

  

논문 [링크](https://arxiv.org/abs/1506.02640)



오타, 오류 지적은 항상 감사히 받겠습니다!



## Abstract

![image.png](https://ifh.cc/g/JNYGlM.jpg)


부제인 "**Unified, Real-Time Object Detection**" 에서 모델의 특징을 집약적으로 표현했습니다. *Abstract*의 내용과 대응해보면 대충 이러한 내용을 말하는 듯 합니다

- Unified  

단일 신경망을 통해 단 한번의 계산(evaluation)으로 전체 이미지에서 bounding boxes(이하 bbox)의 좌표와, class probabilities을 도출할 수 있습니다.  

![R-CNN의 two-stage-detector](https://www.researchgate.net/profile/Neeraj-Bokde/publication/341099304/figure/fig4/AS:888908552359938@1588943722119/RCNN-architecture-17.ppm "R-CNN의 two-stage-detector")  

논문을 읽다보면 이 부분을 상당히 반복적으로 강조하는데, YOLO 이전의 객체 탐지 모델인 R-CNN이나 DPM(Deformable Parts Models)의 경우 다양한 파이프라인과 알고리즘을 섞어서 사용했기에 구조가 복잡했습니다.  

YOLO는 이러한 점에서 기존의 모델들과 차별적인 구조를 제시한 셈이죠. 마치 all-in-one 기능성 화장품을 출시한 것처럼요. YOLO와 같은 단일 구조를 **One-Stage-Detector**라고 합니다.  

  

- Real-Time  

위의 구조로 인해, YOLO는 굉장히 빠르다는 강점을 갖습니다. 일반 모델의 경우 실시간 영상을 45fps로 처리할 수 있으며, 간소화된 Fast YOLO의 경우 다른 모델(2016년 기준)mAP의 두 배 가량을 유지하며 155fps의 속도로 이미지를 처리할 수 있었습니다. 정확도를 어느 정도 포기한 댓가였지만 의미있는 결과를 보여준 셈이죠.

  

- Object Detection  

객체탐지는 YOLO 모델을 고안한 목적이지만, 기존의 모델에 비하여 객체의 유무를 찾고 판별하는데 향상된 기능을 보입니다. 작은 물체나 군집 속에서 단일 개체를 찾는 과정에서의 Localization(객체 위치 추적)오류는 다소 있는 편이지만 배경과 객체를 구분하는 점에서 탁월한 성능을 보입니다. 또한 객체 일반화의 과정에서, 자연물을 보고 학습한 결과를 그림 등의 인위적 예술품에서도 적용할 수 있을 정도의 뛰어난 성능을 보입니다. 이러한 점은 모델이 새롭거나 학습되지 않은 데이터를 제시한들, 성공적으로 분류할 수 있는 가능성을 제시합니다. 


## 모델의 구조

앞서 말했듯 YOLO는 신경망 하나로 모든 연산을 수행할 뿐만 아니라, 특징을 뽑아낼 때 이미지의 전체를 활용합니다.  

이 부분을 처음 읽었을 때 들었던 생각은 "어떻게 이미지를 전부 활용하는 것과 속도를 높이는 것이 양립할 수 있을까?"였습니다. 그러나 이러한 생각은 CNN 중심의 관점입니다.

![sliding window](https://929687.smushcdn.com/2633864/wp-content/uploads/2014/10/sliding_window_example.gif?lossy=1&strip=1&webp=1)  

CNN부터 YOLO 직전의 R-CNN까지 모델들은 위와 같은 sliding window의 방식으로 특징을 뽑아냅니다.  

이 과정에서 window의 stride(보폭)를 늘리면 특징을 듬성듬성 뽑아내게 되고, 반대로 보폭을 줄이면 특징을 세세하게 뽑아낼 수 있지만 시간을 많이 소요하는 딜레마에 빠지게 됩니다.  



### Bounding box prediction

*Abstract*에서 YOLO를 다음과 같이 한 바 있습니다.

> "~ object detection as a **regression** problem to **spatially seperated bounding boxes** and **associated class probabilities**."  

>

객체탐지를 위해 공간 단위로 구분된 bounding box와, 이에 연관된 분류의 확률을 "회귀"의 관점에서 바라본다는 접근입니다.  

이 과정을 그림을 통해 표현하면 다음과 같습니다.  

![bbox regression](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaWsjr%2FbtqVsxhJ8BD%2F54ZVdkz1YoNduB1bXkIyb1%2Fimg.png)  

Bounding box 예측은 다음의 순서에 따라 이루어집니다.

1. 입력된 이미지를 `S`×`S`규격의 격자로 분할한다.

2. 이미지 전체에서 bounding box를 찾는다. 여기에는 총 다섯 가지 정보를 갖는다.

    - box의 중심좌표(`x`, `y`),

    - 높이와 너비(`h`, `w`),

    - confidence score(범위 내 객체가 있을 가능성이 높을 때 1, 아닐 경우 0)

3. 객체의 중심$(x, y)$이 격자 내부에 있을 경우, 격자에서 해당 객체를 탐지한다.

4. 각 격자에서 bouning box B개와 이에 해당하는 confidence score을 예측한다.

  - 이 때 confidence score 계산은 다음 공식에 의해 구할 수 있다.  

$$confidence = Pr(object)*IOU(pred, truth)$$

  - IOU(Intersection over Union)값은 아래 그림과 같이 구한다.  

![IOU](https://blog.kakaocdn.net/dn/I9MIb/btq9eMfNYbF/KeQxOsQydbNkZuRNhoMv9k/img.png)

5. 각 격자에서는 C개의 conditional class probability도 구한다.

    - $Pr(Class_i|Object)$의 형태로 표현하는데, 이 수식을 조건부 확률로 해석한다면 다음과 같다

    > "$Object$가 존재할 때, 이것이 $i$번째 $Class$에 속할 확률"  

    - 격자 한 개 안에 여러 boundong box의 중심이 포착되더라도, 오직 대표되는 단 하나의 bounding box(가장 IOU값이 큰 개체)에 대해서만 분류 확률을 계산한다.

6. Test를 진행할 때, 4.에서 제시된 box의 개별 confidence score 예측치와 5.의 conditional class probability를 곱한다.

$$Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred} = Pr(Class_i)*IOU^{truth}_{pred}$$

여기서 곱해진 결과를 class-specific confidence score이라 하며 다음 두 가지 의미를 내포한다.

    - 예상한 class가 box 안에 존재할 확률

    - Bounding box의 범위가 객체의 위치를 포착해내는 정확도

7. 위 모든 연산을 tensor의 형태로 반환한다. 이 때, tensor의 크기는(`S` x `S` x (`B` * 5 + `C`)이다. 논문에서는 `S=7, B=2, C=20`으로 7 x 7 x 30 크기의 tensor을 반환하였다.


### 신경망의 구조

![YOLO architecture](https://velog.velcdn.com/images%2Fsuminwooo%2Fpost%2Fb9fbe495-9636-4d79-b0e4-97df1dbba0b5%2Fimage.png)

YOLO의 신경망은 크게 두 부분으로 구성되어있습니다. 논문에 의하면 모델의 구조는 GoogLeNet에서 영향을 받았다고 하는군요.

- 이미지의 feature extraction을 담당하는 24개의 convolutional layer(`Conv.Layer`, 제일 왼쪽부터 7×7×1024 블록까지)

- Bounding box와 class를 예측하는 fully

connected layer(`Conn.Layer`)  



### 모델 훈련

모델에 사용된 convolutional layer의 경우 ImageNet 1000-class 데이터셋으로 사전학습되었습니다.   

객체탐지는 정교한 이미지일수록 그 결과가 우수하기 때문에 입력 이미지의 사이즈를 448×448로 확장시킵니다.    

마지막 층에서는 class probabilty와 bounding box의 좌표를 모두 예측하는데, box의 높이와 너비를 정규화하여 [0, 1] 범위 내의 값으로 표현합니다.  

마찬가지로 box 중심의 좌표인 `x`와 `y`도 이를 포함하는 격자 내부에서의 위치를 표현(offset)하기 위해 [0, 1]사이의 값으로 나타냅니다. 마지막을 제외한 모든 은닉층의 끝에는 leaky ReLU를 활성화 함수로 사용합니다. 반면 마지막 층에는 선형 활성화 함수를 사용하는 것이 차이점입니다.


### 손실 함수(Loss function)

위 모델에서는 최적화 과정에서 Sum-Squared Error을 사용합니다. 최적화가 쉽다는 강점이 있기 때문이죠 .

$$SSE = Σ(Y_i - \hat Y_{i})^2$$

다만 이 방식에는 꽤나 많은 단점이 보이네요. 하나씩 열거해보면,

1. Localization error와 classification error 사이에 차등을 두지 않음
2. 이미지를 분할하는 격자 중 대부분은 객체의 중심을 포함하지 않음(`confidence == 0`). 그러나 위와 같은 격자가 많아질 수록, 객체가 있다고 판단된(`confidence score != 0`) 격자마저 이에 영향을 받는(overpowered) 상황이 발생됨 
3. Bounding box의 크기가 상이함에도 불구하고, 큰 box의 오류와 작은 box의 오류에 동일한 가중치를 부여
  
위의 문제점들은 모델을 불안정하게 만듭니다. 연구자들은 이를 해결하기 위해 다음과 같은 개선안을 제시합니다.
- 1, 2번 문제를 해결하기 위해 bounding box 좌표 예측 과정에서 발생하는 손실(`coord`)에는 더 큰 가중치(5)를, 객체가 없다고 판단(`confidence == 0`)하는 과정의 손실(`noobj`)에는 더 작은 가중치를(0.5) 부여함. 즉 다음과 같은 parameter 두 개를 생성

$$λ_{coord} = 5, λ_{noobj} = 0.5$$

- 3번 문제를 해결하기 위해 bounding box의 너비와 높이(`w`, `h`)에 제곱근을 취함(박스 간 편차를 줄임)


